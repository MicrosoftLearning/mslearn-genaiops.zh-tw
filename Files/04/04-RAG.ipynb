{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestrate a RAG system\n",
    "\n",
    "In this notebook, you'll ingest and preprocess data, create embeddings, build a vector store and index, ultimately enabling you to implement a RAG system effectively.\n",
    "\n",
    "## Before you start\n",
    "\n",
    "Install the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain-text-splitters langchain-community langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize components\n",
    "\n",
    "Now you need to define the authentication values that will be used when submitting embeddings and chat completion requests through the API endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the base URL for your Azure OpenAI Service endpoint\n",
    "# Replace 'Your Azure OpenAI Service Endpoint' with your actual endpoint URL obtained previously\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = 'Your Azure OpenAI Service Endpoint'\n",
    "\n",
    "# Define the API key for your Azure OpenAI Service\n",
    "# Replace 'Your Azure OpenAI Service API Key' with your actual API key obtained previously\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = 'Your Azure OpenAI Service API Key'\n",
    "\n",
    "# Define the API version to use for the Azure OpenAI Service\n",
    "os.environ[\"OPENAI_API_VERSION\"] = '2024-08-01-preview'\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"false\" # This will deactivate the logging traces feature of LangChain as it is not required in this exercise\n",
    "\n",
    "# Define the names of the models deployed in your Azure OpenAI Service\n",
    "llm_name = 'gpt-4'\n",
    "embeddings_name = 'text-embedding-ada-002'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you need to initialize the components that will be used from LangChain's suite of integrations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_community.vectorstores import InMemoryVectorStore\n",
    "\n",
    "llm = AzureChatOpenAI(azure_deployment=llm_name)\n",
    "embeddings = AzureOpenAIEmbeddings(azure_deployment=embeddings_name)\n",
    "vector_store = InMemoryVectorStore(embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create indexing pipeline\n",
    "\n",
    "First, you need to load your dataset to begin the indexing process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import CSVLoader\n",
    "\n",
    "loader = CSVLoader(file_path='./app_hotel_reviews.csv',\n",
    "    csv_args={\n",
    "    'delimiter': ',',\n",
    "    'fieldnames': ['Hotel Name', 'User Review']\n",
    "})\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "print(docs[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you need to split the documents into chunks for embedding and vector storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20,\n",
    "    add_start_index=True,\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split documents into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to embed the contents of each text chunk and insert these embeddings into a vector store so that you can search over them to retrieve relevant documents during query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "print(document_ids[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve documents and generate answers\n",
    "\n",
    "Now you can test the RAG application. It will take a user question, search for documents relevant to that question, pass the retrieved documents and initial question to a model, and return an answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "question = \"Where can I stay in London?\"\n",
    "\n",
    "retrieved_docs = vector_store.similarity_search(question)\n",
    "docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "prompt = prompt.invoke({\"question\": question, \"context\": docs_content})\n",
    "answer = llm.invoke(prompt)\n",
    "\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this exercise you built a typical RAG system with its main components. By using your own documents to inform a model's responses, you provide grounding data used by the LLM when it formulates a response. For an enterprise solution, that means that you can constrain generative AI to your enterprise content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "If you've finished the exercise, you should delete the resources you have created to avoid incurring unnecessary Azure costs.\n",
    "\n",
    "1. Return to the browser tab containing the Azure portal (or re-open the [Azure portal](https://portal.azure.com?azure-portal=true) in a new browser tab) and view the contents of the resource group where you deployed the resources used in this exercise.\n",
    "1. On the toolbar, select **Delete resource group**.\n",
    "1. Enter the resource group name and confirm that you want to delete it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
